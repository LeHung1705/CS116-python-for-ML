{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5d0d132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import polars as pl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbbdeb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load(\"train_data/model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a1ca62a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total customers: 2442305\n"
     ]
    }
   ],
   "source": [
    "# Sử dụng lazy loading để không load hết vào RAM\n",
    "df_lazy = pl.scan_parquet(\"train_data/inference_final.parquet\")\n",
    "\n",
    "# Lấy danh sách unique customers\n",
    "unique_customers = df_lazy.select(\"customer_id\").unique().collect().to_series().to_list()\n",
    "print(f\"Total customers: {len(unique_customers)}\")\n",
    "\n",
    "feature_cols = [\n",
    "    # Feature 1: Frequency\n",
    "    'feat1_customer_item_freq',\n",
    "    # Feature 2: Recency Decay\n",
    "    'feat2_brand_affinity', 'feat2_type_affinity',\n",
    "    # Feature 3: Urgency (Window-Based)\n",
    "    'feat3_dist_to_window_center', 'feat3_is_in_window',\n",
    "    # Feature 4: Popularity\n",
    "    'feat4_pop_30d_log', 'feat4_pop_trend', 'feat4_pop_category_rank', 'feat4_pop_global_rank',\n",
    "    # Feature 5: Baby Age Alignment\n",
    "    'feat5_score_age_end_hist', 'feat5_score_age_midpoint',\n",
    "    # Feature 6: Price Compatibility\n",
    "    'feat6_price_compatibility', 'feat6_is_above_user_capacity',\n",
    "    # Feature 7: Brand Loyalty\n",
    "    'feat7_brand_repeat_rate', 'feat7_brand_rank', 'feat7_user_brand_affinity',\n",
    "    # Feature 8: Co-purchase\n",
    "    'feat8_co_purchase_max', 'feat8_co_purchase_sum', 'feat8_co_purchase_count'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45c7d80d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2442305 customers in 25 batches\n",
      "\n",
      "==================================================\n",
      "Processing batch 1/25\n",
      "Batch size: 10000681 rows for 100000 customers\n",
      "Batch 1 completed\n",
      "\n",
      "==================================================\n",
      "Processing batch 2/25\n",
      "Batch size: 10002744 rows for 100000 customers\n",
      "Batch 2 completed\n",
      "\n",
      "==================================================\n",
      "Processing batch 3/25\n",
      "Batch size: 10008220 rows for 100000 customers\n",
      "Batch 3 completed\n",
      "\n",
      "==================================================\n",
      "Processing batch 4/25\n",
      "Batch size: 10008190 rows for 100000 customers\n",
      "Batch 4 completed\n",
      "\n",
      "==================================================\n",
      "Processing batch 5/25\n",
      "Batch size: 10005615 rows for 100000 customers\n",
      "Batch 5 completed\n",
      "\n",
      "==================================================\n",
      "Processing batch 6/25\n",
      "Batch size: 10002229 rows for 100000 customers\n",
      "Batch 6 completed\n",
      "\n",
      "==================================================\n",
      "Processing batch 7/25\n",
      "Batch size: 10006186 rows for 100000 customers\n",
      "Batch 7 completed\n",
      "\n",
      "==================================================\n",
      "Processing batch 8/25\n",
      "Batch size: 10004131 rows for 100000 customers\n",
      "Batch 8 completed\n",
      "\n",
      "==================================================\n",
      "Processing batch 9/25\n",
      "Batch size: 10003239 rows for 100000 customers\n",
      "Batch 9 completed\n",
      "\n",
      "==================================================\n",
      "Processing batch 10/25\n",
      "Batch size: 10004926 rows for 100000 customers\n",
      "Batch 10 completed\n",
      "\n",
      "==================================================\n",
      "Processing batch 11/25\n",
      "Batch size: 10004452 rows for 100000 customers\n",
      "Batch 11 completed\n",
      "\n",
      "==================================================\n",
      "Processing batch 12/25\n",
      "Batch size: 10005613 rows for 100000 customers\n",
      "Batch 12 completed\n",
      "\n",
      "==================================================\n",
      "Processing batch 13/25\n",
      "Batch size: 10002534 rows for 100000 customers\n",
      "Batch 13 completed\n",
      "\n",
      "==================================================\n",
      "Processing batch 14/25\n",
      "Batch size: 10007944 rows for 100000 customers\n",
      "Batch 14 completed\n",
      "\n",
      "==================================================\n",
      "Processing batch 15/25\n",
      "Batch size: 10006797 rows for 100000 customers\n",
      "Batch 15 completed\n",
      "\n",
      "==================================================\n",
      "Processing batch 16/25\n",
      "Batch size: 10009241 rows for 100000 customers\n",
      "Batch 16 completed\n",
      "\n",
      "==================================================\n",
      "Processing batch 17/25\n",
      "Batch size: 10008258 rows for 100000 customers\n",
      "Batch 17 completed\n",
      "\n",
      "==================================================\n",
      "Processing batch 18/25\n",
      "Batch size: 10003002 rows for 100000 customers\n",
      "Batch 18 completed\n",
      "\n",
      "==================================================\n",
      "Processing batch 19/25\n",
      "Batch size: 10006649 rows for 100000 customers\n",
      "Batch 19 completed\n",
      "\n",
      "==================================================\n",
      "Processing batch 20/25\n",
      "Batch size: 10005089 rows for 100000 customers\n",
      "Batch 20 completed\n",
      "\n",
      "==================================================\n",
      "Processing batch 21/25\n",
      "Batch size: 10001767 rows for 100000 customers\n",
      "Batch 21 completed\n",
      "\n",
      "==================================================\n",
      "Processing batch 22/25\n",
      "Batch size: 10005048 rows for 100000 customers\n",
      "Batch 22 completed\n",
      "\n",
      "==================================================\n",
      "Processing batch 23/25\n",
      "Batch size: 10006812 rows for 100000 customers\n",
      "Batch 23 completed\n",
      "\n",
      "==================================================\n",
      "Processing batch 24/25\n",
      "Batch size: 10004762 rows for 100000 customers\n",
      "Batch 24 completed\n",
      "\n",
      "==================================================\n",
      "Processing batch 25/25\n",
      "Batch size: 4232629 rows for 42305 customers\n",
      "Batch 25 completed\n",
      "\n",
      "==================================================\n",
      "Merging all predictions...\n",
      "Total customers with predictions: 2442305\n",
      "shape: (5, 2)\n",
      "┌─────────────┬─────────────────────────────────┐\n",
      "│ customer_id ┆ item_id                         │\n",
      "│ ---         ┆ ---                             │\n",
      "│ i32         ┆ list[str]                       │\n",
      "╞═════════════╪═════════════════════════════════╡\n",
      "│ 31470       ┆ [\"7176000000002\", \"20170000000… │\n",
      "│ 32390       ┆ [\"2017000000035\", \"71750000000… │\n",
      "│ 32524       ┆ [\"7176000000002\", \"20170000000… │\n",
      "│ 35281       ┆ [\"2707000000001\", \"68470000000… │\n",
      "│ 35540       ┆ [\"7176000000002\", \"46900000000… │\n",
      "└─────────────┴─────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Xử lý theo batch để tránh hết RAM\n",
    "import gc\n",
    "\n",
    "BATCH_SIZE = 100000  # Số customers mỗi batch (điều chỉnh theo RAM của bạn)\n",
    "n_batches = (len(unique_customers) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "\n",
    "print(f\"Processing {len(unique_customers)} customers in {n_batches} batches\")\n",
    "\n",
    "# List để lưu kết quả từng batch\n",
    "all_predictions = []\n",
    "\n",
    "for batch_idx in range(n_batches):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Processing batch {batch_idx + 1}/{n_batches}\")\n",
    "    \n",
    "    # Lấy customers cho batch này\n",
    "    start_idx = batch_idx * BATCH_SIZE\n",
    "    end_idx = min((batch_idx + 1) * BATCH_SIZE, len(unique_customers))\n",
    "    batch_customers = unique_customers[start_idx:end_idx]\n",
    "    \n",
    "    # Filter data cho batch customers này và collect vào RAM\n",
    "    df_batch = (\n",
    "        df_lazy\n",
    "        .filter(pl.col(\"customer_id\").is_in(batch_customers))\n",
    "        .collect()\n",
    "    )\n",
    "    \n",
    "    print(f\"Batch size: {len(df_batch)} rows for {len(batch_customers)} customers\")\n",
    "    \n",
    "    # Extract features\n",
    "    X_batch = df_batch.select(feature_cols).to_pandas()\n",
    "    \n",
    "    # Dự đoán xác suất\n",
    "    y_pred_proba = model.predict_proba(X_batch)[:, 1]\n",
    "    \n",
    "    # Thêm cột dự đoán vào dataframe\n",
    "    df_batch = df_batch.with_columns(pl.Series(\"pred_score\", y_pred_proba))\n",
    "    \n",
    "    # Sắp xếp và lấy top 10 items cho mỗi customer trong batch này\n",
    "    top_predictions_batch = (\n",
    "        df_batch.sort([\"customer_id\", \"pred_score\"], descending=[False, True])\n",
    "        .group_by(\"customer_id\")\n",
    "        .agg(pl.col(\"item_id\").head(10).alias(\"item_id\"))\n",
    "    )\n",
    "    \n",
    "    all_predictions.append(top_predictions_batch)\n",
    "    \n",
    "    # Clear memory\n",
    "    del df_batch, X_batch, y_pred_proba, top_predictions_batch\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"Batch {batch_idx + 1} completed\")\n",
    "\n",
    "# Merge tất cả kết quả\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"Merging all predictions...\")\n",
    "top_predictions = pl.concat(all_predictions)\n",
    "\n",
    "print(f\"Total customers with predictions: {len(top_predictions)}\")\n",
    "print(top_predictions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d7c2d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã lưu predictions cho 2442305 customers vào pred.json\n"
     ]
    }
   ],
   "source": [
    "# Chuyển đổi sang format JSON và lưu file\n",
    "import json\n",
    "\n",
    "predictions_dict = {}\n",
    "for row in top_predictions.iter_rows(named=True):\n",
    "    customer_id = str(row[\"customer_id\"])\n",
    "    item_ids = [str(iid) for iid in row[\"item_id\"]]\n",
    "    predictions_dict[customer_id] = item_ids\n",
    "\n",
    "# Lưu ra file pred.json\n",
    "with open(\"pred.json\", \"w\") as f:\n",
    "    json.dump(predictions_dict, f, indent=2)\n",
    "\n",
    "print(f\"Đã lưu predictions cho {len(predictions_dict)} customers vào pred.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9630b84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(pred, gt, hist, filter_bought_items=True, K=10): # prediction, ground-truth, history items, candidate items\n",
    "    precisions = []\n",
    "    ideal_precs = []\n",
    "    ncold_start = 0\n",
    "    cold_start_users = []\n",
    "    nusers = len(gt.keys())\n",
    "    for user in gt.keys():\n",
    "        if (user not in hist) or (user not in pred):\n",
    "            ncold_start += 1\n",
    "            cold_start_users.append(user) # THINKING: để giảm cold start có thể tăng khoảng HISTORY\n",
    "            continue\n",
    "        gt_items = gt[user]\n",
    "        relevant_items = set(gt_items)\n",
    "        if filter_bought_items:\n",
    "            relevant_items -=set(hist[user])\n",
    "        # Compute precision@k\n",
    "        hits = len(set(pred[user][:K]) & relevant_items)\n",
    "        precisions.append(hits / K)\n",
    "    return np.mean(precisions), cold_start_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8bda54e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2442305 predictions\n",
      "Loaded 391900 ground truth customers\n",
      "Loaded 2442305 customers with history\n"
     ]
    }
   ],
   "source": [
    "# Load các argument để tính precision\n",
    "import json\n",
    "import joblib\n",
    "import polars as pl\n",
    "\n",
    "# 1. Load predictions từ pred.json\n",
    "with open(\"pred.json\", \"r\") as f:\n",
    "    pred = json.load(f)\n",
    "\n",
    "# 2. Load ground truth\n",
    "gt = joblib.load(\"train_data/groundtruth.pkl\")\n",
    "\n",
    "# 3. Load history items từ processed_purchase.parquet\n",
    "purchase_df = pl.read_parquet(\"processed_data/processed_purchase.parquet\")\n",
    "\n",
    "# Tạo dictionary history: customer_id -> list of item_ids (dùng Polars group_by)\n",
    "hist_df = (\n",
    "    purchase_df\n",
    "    .group_by(\"customer_id\")\n",
    "    .agg(pl.col(\"item_id\").alias(\"item_ids\"))\n",
    ")\n",
    "\n",
    "# Chuyển sang dictionary\n",
    "hist = {}\n",
    "for row in hist_df.iter_rows(named=True):\n",
    "    customer_id = str(row[\"customer_id\"])\n",
    "    item_ids = [str(iid) for iid in row[\"item_ids\"]]\n",
    "    hist[customer_id] = item_ids\n",
    "\n",
    "print(f\"Loaded {len(pred)} predictions\")\n",
    "print(f\"Loaded {len(gt)} ground truth customers\")\n",
    "print(f\"Loaded {len(hist)} customers with history\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c42424a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert gt keys từ int sang str để match với pred và hist\n",
    "gt = {str(k): [str(item) for item in v] for k, v in gt.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d78b05b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@10: 0.0158\n",
      "Number of cold start users: 58180\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Tính precision@10\n",
    "precision, cold_start_users = precision_at_k(pred, gt, hist, filter_bought_items=True, K=10)\n",
    "\n",
    "print(f\"Precision@10: {precision:.4f}\")\n",
    "print(f\"Number of cold start users: {len(cold_start_users)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
